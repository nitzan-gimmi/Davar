
---

ðŸ“˜ README.md

`markdown

HFBT: Hebrew Fusion-Based Tokenizer

HFBT (Hebrew Fusion-Based Tokenizer) is a morphological tokenizer designed for Hebrew. It decomposes words into linguistic componentsâ€”prefixes, roots, patterns, and suffixesâ€”enabling efficient, semantically rich tokenization for NLP tasks.

ðŸ” Overview

- Language: Hebrew
- Approach: Morphology-based (root + pattern + affixes)
- Format: JSONL output with full morphological breakdown
- Compatibility: HuggingFace Tokenizer class, shell pipelines, ASR/NLP systems

ðŸ“¦ Project Structure

`
HFBT/
â”œâ”€â”€ data/                # Raw, cleaned, and tokenized corpora
â”œâ”€â”€ scripts/             # Preprocessing and tokenization scripts
â”œâ”€â”€ hfbt/                # Morphological engine and rules
â”œâ”€â”€ training/            # Model training configs and checkpoints
â”œâ”€â”€ evaluation/          # Evaluation scripts and error analysis
â”œâ”€â”€ reproducibility/     # Docker, environment, seeds, example runs
â”œâ”€â”€ docs/                # Documentation, diagrams, methodology
`

ðŸš€ Quick Start

`bash
bash scripts/preprocess.sh
`

Or run tokenizer directly:

`bash
python3 scripts/tokenize.py --input data/cleancorpus.txt --output data/tokenizedhfbt.jsonl
`

ðŸ§  Tokenizer Example

Input: "×•×›×©×™×ª×¤×œ×œ×•"  
Output:  
`json
{
  "token": "×•×›×©×™×ª×¤×œ×œ×•",
  "root": "×¤.×œ.×œ",
  "template": "×”×ª×¤×¢×œ",
  "morphemes": ["×•", "×›×©", "×”×œ×š", "× ×•× "]
}
`

ðŸ“Š Performance Highlights

| Metric         | Baseline | HFBT   | Gain     |
|----------------|----------|--------|----------|
| Precision      | 0.60     | 0.95   | +58.3%   |
| Token Count    | 302,572  | 76,211 | -74.8%   |
| Vocabulary     | 29,136   | 8,241  | -71.7%   |
| WER            | 40%      | 5%     | -87.5%   |
| Cost (1K words)| $10.00   | $1.25  | -87.5%   |

ðŸ§¬ Philosophy

HFBT is built on the belief that language is not just dataâ€”itâ€™s structure, intention, and evolution. The â€œVector of Willâ€ and â€œSpiral of Knowledgeâ€ are conceptual tools that guide our design, measuring not just performance, but direction and depth.

ðŸ“œ License

Apache 2.0 for code, CC-BY for data.

ðŸ™Œ Citation

See CITATION.cff for academic reference.
`

---

ðŸ“˜ methodology.md

`markdown

HFBT Methodology

1. Corpus Description

- Sources: Hebrew news, literature, speech transcripts, public texts
- Licensing: CC-BY, CC0
- Size: 1.2B tokens (~150M sentences)
- Split: 90/5/5 (train/val/test), stratified by genre
- Filtering:
  - Remove non-Hebrew content >30%
  - Deduplication >95% similarity
  - Boilerplate removal (headers, footers)
- Seed: 42 (for reproducibility)

2. Preprocessing

- Unicode normalization: NFC
- Whitespace and punctuation cleanup
- Diacritics: removed unless needed for TTS
- Speech: downsample to 16kHz, RMS normalization
- Mapping: URLs â†’ <URL>, numbers â†’ <NUM>, emojis â†’ <EMOJI>

3. Tokenization Algorithm (HFBT)

- Input: Hebrew sentence
- Steps:
  1. Normalize and segment
  2. Identify prefixes/suffixes via rule hierarchy
  3. Extract root using dictionary or heuristics
  4. Detect pattern (e.g., ×¤×¢×œ, ×”×ª×¤×¢×œ)
  5. Output tokens: [prefix, root, pattern, suffix]
- OOV Handling: UNKROOT + charngrams(3)
- Vocabulary: frequency cutoff = 5

4. Baselines

Compared against:
- BPE (32k)
- WordPiece
- AlephBERT tokenizer
- DictaLM tokenizer

5. Training Setup

- Architecture: Transformer-Base (12 layers, 768 hidden)
- Optimizer: AdamW (Î²1=0.9, Î²2=0.95), weight_decay=0.01
- LR: warmup 10k steps, peak 5e-4, cosine decay
- Precision: FP16
- Epochs: 200k steps
- Validation: every 5k steps, early stopping (patience=5)
- Seeds: [42, 123, 2025]

6. Evaluation Metrics

- ASR: WER, CER, substitution/insertion/deletion breakdown
- LM: Perplexity
- Downstream: F1, Exact Match, BLEU, ROUGE
- Efficiency: tokens/sec, memory footprint, latency, cost/query

7. Robustness Testing

- OOD: slang, biblical, legal texts
- Noise: typos, spelling errors, random insertions
- Ablation: remove root/prefix/suffix handling â†’ measure impact

8. Error Analysis

- Categories: hallucinations, segmentation errors, root mismatches
- Manual review: 30+ examples per category
- Confusion matrix: by error type
- Fixes: dictionary updates, rule tuning, weighted loss

9. Ethics & Transparency

- PII removal: anonymization pipeline
- Bias analysis: genre, dialect, demographic coverage
- Licensing: open-source code (Apache 2.0), data (CC-BY)
- Release: tokenizer class, pretrained weights, API access

10. Reproducibility Package

- Scripts: preprocess.sh, tokenize.py, train.py, evaluate.py
- Configs: config.yaml, training_config.json
- Docker: Dockerfile, environment.yml
- Seeds: seeds.txt
- Example run: example_run.sh
- Cards: modelcard.md, datasetcard.md
`

---
